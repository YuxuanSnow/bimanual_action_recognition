{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### How to preprocess data\n",
    "\n",
    "In our case, the node should be each hand and object. The feature of the node should be bounding box / extracted features\n",
    "(tbd). The edges between node should be undirected. The features of edge should be distance between node or others (tbd).\n",
    "The label of each node should be: action id for hand and relative action, idle id for other unused object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import traceback\n",
    "from typing import List, Tuple, Dict, Generator, Any\n",
    "\n",
    "objects = ['bowl', 'knife', 'screwdriver', 'cuttingboard', 'whisk', 'hammer', 'bottle', 'cup',\n",
    "           'banana', 'cereals', 'sponge', 'woodenwedge', 'saw', 'harddrive', 'left_hand',\n",
    "           'right_hand']\n",
    "relations = ['contact', 'above', 'below', 'left of', 'right of', 'behind of', 'in front of',\n",
    "             'inside', 'surround', 'moving together', 'halting together', 'fixed moving together',\n",
    "             'getting close', 'moving apart', 'stable', 'temporal']\n",
    "actions = ['idle', 'approach', 'retreat', 'lift', 'place', 'hold', 'pour', 'cut', 'hammer', 'saw',\n",
    "           'stir', 'screw', 'drink', 'wipe']\n",
    "\n",
    "\n",
    "def load(basepath: str, dataset_config: str, evaluation_mode: str = 'normal', filter_if=lambda x: False):\n",
    "    datasets_basepath = os.path.join(basepath, 'dataset_caches')\n",
    "    paths = get_dataset_paths(datasets_basepath, dataset_config)\n",
    "    # Instantiate all scene graph proxies.\n",
    "    dataset = [SceneGraphProxy(p, evaluation_mode) for p in paths]\n",
    "    # Filter scene graph proxies by given function.\n",
    "    filtered_dataset: List[SceneGraphProxy] = [x for x in dataset if not filter_if(x)]\n",
    "    return filtered_dataset\n",
    "\n",
    "\n",
    "def get_dataset_paths(dataset_basepath: str, dataset_config: str) -> List[str]:\n",
    "    paths: List[str] = []\n",
    "\n",
    "    for subject, task, take in crawl_dataset():\n",
    "        rec_basepath = os.path.join(dataset_basepath, dataset_config, subject, task, take)\n",
    "        frame_count: int = len(os.listdir(rec_basepath)) // 2\n",
    "        for i in range(frame_count):\n",
    "            paths.append(os.path.join(rec_basepath, 'frame_{}_left.cache'.format(i)))\n",
    "            paths.append(os.path.join(rec_basepath, 'frame_{}_right.cache'.format(i)))\n",
    "\n",
    "    paths.sort()\n",
    "\n",
    "    return paths\n",
    "\n",
    "\n",
    "def get_raw_dataset_paths(derived_data_basepath, ground_truth_basepath: str) -> Tuple[List[str], List[str]]:\n",
    "    derived_data_paths: List[str] = []\n",
    "    ground_truth_paths: List[str] = []\n",
    "    # list contains path to the derived and groundtruth data\n",
    "\n",
    "    for subject, task, take in crawl_dataset():\n",
    "        derived_data_paths.append(os.path.join(derived_data_basepath, subject, task, take))\n",
    "        ground_truth_paths.append(os.path.join(ground_truth_basepath, subject, task, take + '.json'))\n",
    "\n",
    "    # derived_data_path: a path to the dataset\n",
    "    # derived_data_paths: a list contains paths to dataset, with specific subject, task, and take\n",
    "\n",
    "    return derived_data_paths, ground_truth_paths\n",
    "\n",
    "\n",
    "def crawl_dataset() -> Generator[Tuple[str, str, str], None, None]:\n",
    "    for subject in ['subject_{}'.format(i) for i in range(1, 7)]:\n",
    "        for task in ['task_1_k_cooking', 'task_2_k_cooking_with_bowls', 'task_3_k_pouring',\n",
    "                     'task_4_k_wiping', 'task_5_k_cereals', 'task_6_w_hard_drive',\n",
    "                     'task_7_w_free_hard_drive', 'task_8_w_hammering', 'task_9_w_sawing']:\n",
    "            for take in ['take_{}'.format(take_i) for take_i in range(10)]:\n",
    "                yield subject, task, take\n",
    "                # yield: function will return a generator\n",
    "\n",
    "\n",
    "class BoundingBox:\n",
    "\n",
    "    def __init__(self, x0: float = 0, x1: float = 0, y0: float = 0, y1: float = 0, z0: float = 0, z1: float = 0,\n",
    "                 serialised_bb: Dict[str, Any] = None):\n",
    "        self.x0: float = x0\n",
    "        self.x1: float = x1\n",
    "        self.y0: float = y0\n",
    "        self.y1: float = y1\n",
    "        self.z0: float = z0\n",
    "        self.z1: float = z1\n",
    "        if serialised_bb is not None:\n",
    "            for a in ['x0', 'x1', 'y0', 'y1', 'z0', 'z1']:\n",
    "                setattr(self, a, serialised_bb[a])\n",
    "        assert x1 >= x0, '{} < {}'.format(x1, x0)\n",
    "        assert y1 >= y0, '{} < {}'.format(y1, y0)\n",
    "        assert z1 >= z0, '{} < {}'.format(z1, z0)\n",
    "\n",
    "    def to_float_tuple(self) -> Tuple[float, float, float, float, float, float]:\n",
    "        return self.x0, self.x1, self.y0, self.y1, self.z0, self.z1\n",
    "\n",
    "\n",
    "class Object:\n",
    "\n",
    "    def __init__(self, certainty: float = 0, class_index: int = -1, class_name: str = '', instance_name: str = '',\n",
    "                 bounding_box: BoundingBox = BoundingBox(), past_bounding_box: BoundingBox = BoundingBox(),\n",
    "                 serialised_object: Dict[str, Any] = None):\n",
    "        self.certainty: float = certainty\n",
    "        self.class_index: int = class_index\n",
    "        self.class_name: str = class_name\n",
    "        self.instance_name: str = instance_name\n",
    "        self.bounding_box: BoundingBox = bounding_box\n",
    "        self.past_bounding_box: BoundingBox = past_bounding_box\n",
    "        if serialised_object is not None:\n",
    "            for a in ['certainty', 'class_index', 'class_name', 'instance_name']:\n",
    "                setattr(self, a, serialised_object[a])\n",
    "            for a in ['bounding_box', 'past_bounding_box']:\n",
    "                setattr(self, a, BoundingBox(serialised_bb=serialised_object[a]))\n",
    "            if self.class_name == 'RightHand':\n",
    "                self.class_index = objects.index('right_hand')\n",
    "            elif self.class_name == 'LeftHand':\n",
    "                self.class_index = objects.index('left_hand')\n",
    "\n",
    "\n",
    "class Relation:\n",
    "\n",
    "    def __init__(self, subject_index: int = -1, object_index: int = -1, relation_name: str = '',\n",
    "                 serialised_relation: Dict[str, Any] = None):\n",
    "        self.subject_index: int = subject_index\n",
    "        self.object_index: int = object_index\n",
    "        self.relation_name: str = relation_name\n",
    "        if serialised_relation is not None:\n",
    "            for a in ['subject_index', 'object_index', 'relation_name']:\n",
    "                setattr(self, a, serialised_relation[a])\n",
    "\n",
    "\n",
    "class GroundTruth:\n",
    "\n",
    "    def __init__(self, gtlist):\n",
    "        self.gtlist = gtlist\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        for i in range(0, len(self.gtlist) - 1, 2):\n",
    "            begin = self.gtlist[i]\n",
    "            action_id = self.gtlist[i + 1]\n",
    "            end = self.gtlist[i + 2]\n",
    "            if begin <= k < end:\n",
    "                return action_id\n",
    "        if k == self.gtlist[-1]:\n",
    "            return self.gtlist[-2]\n",
    "        raise IndexError()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.gtlist[-1] + 1\n",
    "\n",
    "\n",
    "def one_hot_encode(length: int, elements: List[int]) -> List[float]:\n",
    "    # Definition one_hot_encoding: a one-hot is a group of bits among which the legal combinations of values are only\n",
    "    # those with a single high (1) bit and all the others low (0).\n",
    "    if not isinstance(length, int) or length < 0:\n",
    "        raise ValueError('length must be an int <= 0')\n",
    "    if not isinstance(elements, list):\n",
    "        raise ValueError('elements must be a list')\n",
    "    output = [0.] * length\n",
    "    for element in elements:\n",
    "        if not isinstance(element, int) or not (0 <= element < length):\n",
    "            raise ValueError('All elements must be an int with: 0 <= element < length')\n",
    "        output[element] = 1.\n",
    "    return output\n",
    "\n",
    "\n",
    "class SceneGraph:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.right_action: int = -1\n",
    "        self.left_action: int = -1\n",
    "        self.nodes: Dict[int, Tuple[int, str, Tuple[float, float, float, float, float, float]]] = {}\n",
    "        self.edges: Dict[Tuple[int, int], List[int]] = {}\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, SceneGraph):\n",
    "            return (self.right_action == other.right_action and self.left_action == other.left_action and\n",
    "                    self.nodes == other.nodes and self.edges == other.edges)\n",
    "        return False\n",
    "\n",
    "    def check_integrity(self):\n",
    "        assert self.right_action is None or (0 <= self.right_action < len(actions))\n",
    "        assert self.left_action is None or (0 <= self.left_action < len(actions))\n",
    "        for node_id in range(0, len(self.nodes), 1):\n",
    "            assert node_id in self.nodes\n",
    "\n",
    "    def to_data_dict(self, mirrored=False) -> Dict[str, List]:\n",
    "        action_id: int = self.right_action\n",
    "        if mirrored:\n",
    "            action_id = self.left_action\n",
    "        graph_globals: List[float] = one_hot_encode(len(actions), [] if action_id is None else [action_id])\n",
    "\n",
    "        graph_nodes: List[List[float]] = []\n",
    "        for node_id, (class_id, _, bb) in self.nodes.items():\n",
    "            if mirrored:\n",
    "                class_id = SceneGraph._mirror_hands(class_id)\n",
    "            graph_nodes.append(one_hot_encode(len(objects), [class_id]) + list(bb))\n",
    "\n",
    "        graph_edges: List[List[float]] = []\n",
    "        graph_senders: List[int] = []\n",
    "        graph_receivers: List[int] = []\n",
    "        for (sender, receiver), relations in self.edges.items():\n",
    "            if mirrored:\n",
    "                relations = [SceneGraph._mirror_relations(r) for r in relations]\n",
    "            graph_edges.append(one_hot_encode(len(relations), relations))\n",
    "            graph_senders.append(sender)\n",
    "            graph_receivers.append(receiver)\n",
    "\n",
    "        return {\n",
    "            'globals': graph_globals,\n",
    "            'nodes': graph_nodes,\n",
    "            'edges': graph_edges,\n",
    "            'senders': graph_senders,\n",
    "            'receivers': graph_receivers\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _mirror_hands(class_id: int) -> int:\n",
    "        if objects[class_id] == 'right_hand':\n",
    "            return objects.index('left_hand')\n",
    "        if objects[class_id] == 'left_hand':\n",
    "            return objects.index('right_hand')\n",
    "        return class_id\n",
    "\n",
    "    @staticmethod\n",
    "    def _mirror_relations(relation_id: int) -> int:\n",
    "        if relations[relation_id] == 'left of':\n",
    "            return relations.index('right of')\n",
    "        if relations[relation_id] == 'right of':\n",
    "            return relations.index('left of')\n",
    "        return relation_id\n",
    "\n",
    "\n",
    "def flatten_scene_graphs(scene_graph_list: List[SceneGraph]) -> SceneGraph:\n",
    "    temporal_sg = SceneGraph()\n",
    "\n",
    "    # Maps the scene graph id and local node id to a global node id.\n",
    "    global_node_id_map: Dict[Tuple[int, int], int] = {}\n",
    "\n",
    "    # Ground truth of the temporal scene graph is the ground truth of the most recent scene graph (last in list).\n",
    "    temporal_sg.right_action = scene_graph_list[-1].right_action\n",
    "    temporal_sg.left_action = scene_graph_list[-1].left_action\n",
    "\n",
    "    global_node_id: int = 0\n",
    "    # Add the nodes to the temporal scene graph.  Populate global node id map alongside.\n",
    "    for sg_id, sg in enumerate(scene_graph_list):\n",
    "        for node_id, node in sg.nodes.items():\n",
    "            key = (sg_id, node_id)\n",
    "            global_node_id_map[key] = global_node_id\n",
    "            temporal_sg.nodes[global_node_id] = node\n",
    "            global_node_id += 1\n",
    "\n",
    "    # Add the edges to the temporal scene graph.\n",
    "    for sg_id, sg in enumerate(scene_graph_list):\n",
    "        for (sender, receiver), relations in sg.edges.items():\n",
    "            key = (global_node_id_map[sg_id, sender], global_node_id_map[sg_id, receiver])\n",
    "            temporal_sg.edges[key] = relations\n",
    "\n",
    "    # Add the edges for the temporal relations.\n",
    "    for sg_id in range(1, len(scene_graph_list), 1):\n",
    "        sg = scene_graph_list[sg_id]\n",
    "        past_sg = scene_graph_list[sg_id - 1]\n",
    "        for node_id, (_, node_name, _) in sg.nodes.items():\n",
    "            for past_node_id, (_, past_node_name, _) in past_sg.nodes.items():\n",
    "                if node_name == past_node_name:\n",
    "                    # One temporal edge from an object node to the corresponding node one step in the past.\n",
    "                    key = (global_node_id_map[sg_id, node_id], global_node_id_map[sg_id - 1, past_node_id])\n",
    "                    temporal_sg.edges[key] = [relations.index('temporal')]\n",
    "\n",
    "    return temporal_sg\n",
    "\n",
    "\n",
    "class Recording:\n",
    "\n",
    "    def __init__(self, derived_data_path: str = None, ground_truth_path: str = None):\n",
    "        if derived_data_path is None or ground_truth_path is None:\n",
    "            raise ValueError('derived_data_path and ground_truth_path must both be set.')\n",
    "\n",
    "        self.frame_count: int = 0\n",
    "        self.derived_data_path: str = derived_data_path\n",
    "        self.ground_truth_path: str = ground_truth_path\n",
    "        self.objects: List[List[Object]] = []\n",
    "        self.relations: List[List[Relation]] = []\n",
    "        self.ground_truth_left: GroundTruth\n",
    "        self.ground_truth_right: GroundTruth\n",
    "\n",
    "        self.frame_count = len(os.listdir(os.path.join(self.derived_data_path, 'spatial_relations')))\n",
    "        self._load_objects()\n",
    "        self._load_relations()\n",
    "        self._load_ground_truth()\n",
    "\n",
    "    def check_integrity(self) -> None:\n",
    "        # Check sizes.\n",
    "        assert len(self.objects) == self.frame_count\n",
    "        assert len(self.relations) == self.frame_count\n",
    "        assert len(self.ground_truth_right) == self.frame_count\n",
    "        assert len(self.ground_truth_left) == self.frame_count\n",
    "\n",
    "        for frame in range(0, self.frame_count, 1):\n",
    "            # Check objects.\n",
    "            taken_instance_names: List[str] = []\n",
    "            for obj in self.objects[frame]:\n",
    "                assert 0 <= obj.class_index < len(objects)\n",
    "                assert obj.instance_name not in taken_instance_names\n",
    "                taken_instance_names.append(obj.instance_name)\n",
    "            del taken_instance_names\n",
    "\n",
    "            # Check relations.\n",
    "            for rel in self.relations[frame]:\n",
    "                assert rel.subject_index is not None and rel.object_index is not None\n",
    "                assert rel.subject_index != rel.object_index\n",
    "                assert rel.relation_name in relations\n",
    "\n",
    "    def to_scene_graph(self, frame_number: int) -> SceneGraph:\n",
    "        assert 0 <= frame_number < self.frame_count, 'Requested frame out of range'\n",
    "\n",
    "        sg = SceneGraph()\n",
    "\n",
    "        # Annotations (actions).\n",
    "        sg.right_action = self.ground_truth_right[frame_number]\n",
    "        sg.left_action = self.ground_truth_left[frame_number]\n",
    "\n",
    "        # Nodes (objects).\n",
    "        for node_id, obj in enumerate(self.objects[frame_number]):\n",
    "            bb = obj.bounding_box.to_float_tuple()\n",
    "            sg.nodes[node_id] = (obj.class_index, obj.instance_name, bb)\n",
    "\n",
    "        # Edges (relations).\n",
    "        for relation in self.relations[frame_number]:\n",
    "            key = (relation.subject_index, relation.object_index)\n",
    "            if key not in sg.edges:\n",
    "                sg.edges[key] = []\n",
    "            sg.edges[key].append(relations.index(relation.relation_name))\n",
    "\n",
    "        return sg\n",
    "\n",
    "    def to_scene_graphs(self, frame_number: int, history_size: int = 10) -> List[SceneGraph]:\n",
    "        assert 0 <= frame_number < self.frame_count, 'Requested frame out of range'\n",
    "        # for each frame i define a list of Scenegraph\n",
    "        sgl: List[SceneGraph] = []\n",
    "        # here history_size = 10 means 10 scenegraph has to be considered for temporal edge\n",
    "        # if current frame less than 10, than take all frames before into list sgl\n",
    "        for i in range(max(frame_number - history_size + 1, 0), frame_number + 1, 1):\n",
    "            sgl.append(self.to_scene_graph(i))\n",
    "        return sgl\n",
    "\n",
    "    def _load_objects(self):\n",
    "        assert len(self.objects) == 0\n",
    "        for obs in self._load_json_series('3d_objects'):\n",
    "            self.objects.append([Object(serialised_object=ob) for ob in obs])\n",
    "        assert len(self.objects) == self.frame_count\n",
    "\n",
    "    def _load_relations(self):\n",
    "        assert len(self.relations) == 0\n",
    "        for rels in self._load_json_series('spatial_relations'):\n",
    "            self.relations.append([Relation(serialised_relation=rel) for rel in rels])\n",
    "        assert len(self.relations) == self.frame_count\n",
    "\n",
    "    def _load_ground_truth(self):\n",
    "        with open(self.ground_truth_path) as f:\n",
    "            gtjson = json.load(f)\n",
    "            self.ground_truth_left = GroundTruth(gtjson['left_hand'])\n",
    "            self.ground_truth_right = GroundTruth(gtjson['right_hand'])\n",
    "\n",
    "    def _load_json_series(self, type):\n",
    "        def get_path(index):\n",
    "            return os.path.join(self.derived_data_path, type, 'frame_{}.json'.format(index))\n",
    "        for i in range(0, self.frame_count, 1):\n",
    "            loaded_successfully = False\n",
    "            while not loaded_successfully:\n",
    "                try:\n",
    "                    with open(get_path(i)) as of:\n",
    "                        # path example for _load_objects(): derived_data_path + \"3d_objects\" + frame_10\n",
    "                        yield json.load(of)\n",
    "                    loaded_successfully = True\n",
    "                except IOError:\n",
    "                    print(\"Error while loading\")\n",
    "\n",
    "        assert not os.path.isfile(get_path(self.frame_count))\n",
    "\n",
    "\n",
    "class SceneGraphProxy:\n",
    "    def __init__(self, path: str, mode: str):\n",
    "        self.path: str = path\n",
    "        self.subject: int = -1\n",
    "        self.task: int = -1\n",
    "        self.take: int = -1\n",
    "        self.frame: int = -1\n",
    "        self.side: str = 'none'\n",
    "        self.mode: str = mode\n",
    "        assert os.path.isfile(path), 'Not a valid path: {}'.format(path)\n",
    "        for part in self.path.split('/'):\n",
    "            if part.startswith('subject_'):\n",
    "                pos = part.find('subject_') + len('subject_')\n",
    "                self.subject = int(part[pos:pos + 1])\n",
    "            elif part.startswith('task_'):\n",
    "                pos = part.find('task_') + len('task_')\n",
    "                self.task = int(part[pos:pos + 1])\n",
    "            elif part.startswith('take_'):\n",
    "                pos = part.find('take_') + len('take_')\n",
    "                self.take = int(part[pos:pos + 1])\n",
    "            elif part.startswith('frame_'):\n",
    "                if '_left' in part:\n",
    "                    self.side = 'left'\n",
    "                elif '_right' in part:\n",
    "                    self.side = 'right'\n",
    "                self.frame = int(part[len('frame_'):-len('_{}.cache'.format(self.side))])\n",
    "        assert all(p >= 0 for p in [self.subject, self.task, self.take, self.frame])\n",
    "        assert self.side != 'none'\n",
    "\n",
    "    def load(self):\n",
    "        with open(self.path, 'rb') as f:\n",
    "            try:\n",
    "                return getattr(self, 'load_{}'.format(self.mode))(pickle.load(f))\n",
    "            except pickle.UnpicklingError as e:\n",
    "                time.sleep(0.1)\n",
    "                try:\n",
    "                    return getattr(self, 'load_{}'.format(self.mode))(pickle.load(f))\n",
    "                except pickle.UnpicklingError:\n",
    "                    raise Exception('Repeatedly failed to unpickle file.')\n",
    "\n",
    "    def load_contact(self, graph):\n",
    "        graph = self.load_normal(graph)\n",
    "        for i in range(len(graph['edges'])):\n",
    "            for j, rel in enumerate(relations):\n",
    "                if rel != 'contact' and rel != 'temporal':\n",
    "                    graph['edges'][i][j] = 0.  # Censor all relations except for contact and temporal.\n",
    "                    assert graph['edges'][i][j] == 0.\n",
    "        return graph\n",
    "\n",
    "    def load_centroids(self, graph):\n",
    "        for i in range(len(graph['edges'])):\n",
    "            for j, rel in enumerate(relations):\n",
    "                if rel != 'temporal':\n",
    "                    graph['edges'][i][j] = 0.  # Censor all relations except for temporal.\n",
    "                    assert graph['edges'][i][j] == 0.\n",
    "        for i in range(len(graph['nodes'])):\n",
    "            offset = len(objects)\n",
    "            cx = graph['nodes'][i][offset + 1] - graph['nodes'][i][offset]\n",
    "            cy = graph['nodes'][i][offset + 3] - graph['nodes'][i][offset + 2]\n",
    "            cz = graph['nodes'][i][offset + 5] - graph['nodes'][i][offset + 4]\n",
    "            # Cut off bounding box data and replace it with centroid.\n",
    "            graph['nodes'][i] = graph['nodes'][i][:-6] + [cx, cy, cz]\n",
    "            assert len(graph['nodes'][i]) == len(objects) + 3\n",
    "        return graph\n",
    "\n",
    "    def load_normal(self, graph):\n",
    "        for i in range(len(graph['nodes'])):\n",
    "            graph['nodes'][i] = graph['nodes'][i][:-6]  # Cut off bounding box data.\n",
    "            assert len(graph['nodes'][i]) == len(objects)\n",
    "        return graph\n",
    "\n",
    "\n",
    "def load_symbolic(out_path):\n",
    "    cachefile = os.path.join(out_path, 'dataset_caches', 'symbolic_dataset.cache')\n",
    "    with open(cachefile, 'rb') as f:\n",
    "        recs = pickle.load(f)\n",
    "    return recs\n",
    "\n",
    "\n",
    "def symbolic_datset_exists(basepath: str) -> bool:\n",
    "    cachefile = os.path.join(basepath, 'dataset_caches', 'symbolic_dataset.cache')\n",
    "    return os.path.isfile(cachefile)\n",
    "\n",
    "\n",
    "def generate_symbolic_dataset(dataset_path, basepath) -> None:\n",
    "\n",
    "    derived_data = 'bimacs_derived_data'\n",
    "    rgbd_data_ground_truth = 'bimacs_rgbd_data_ground_truth'\n",
    "\n",
    "    os.makedirs(os.path.join(basepath, 'dataset_caches'), exist_ok=True)\n",
    "    cachefile = os.path.join(basepath, 'dataset_caches', 'symbolic_dataset.cache')\n",
    "    recs = {}\n",
    "\n",
    "    derived_data_paths, ground_truth_paths = get_raw_dataset_paths(os.path.join(dataset_path, derived_data),\n",
    "                                             os.path.join(dataset_path, rgbd_data_ground_truth))\n",
    "\n",
    "    for derived_data_path, ground_truth_path in zip(derived_data_paths, ground_truth_paths):\n",
    "        *_, subject, task, take = derived_data_path.split('/')\n",
    "        if subject not in recs:\n",
    "            recs[subject] = {}\n",
    "        if task not in recs[subject]:\n",
    "            recs[subject][task] = {}\n",
    "        rec = Recording(derived_data_path, ground_truth_path)\n",
    "        recs[subject][task][take] = rec\n",
    "\n",
    "    # Write cache file.\n",
    "    with open(cachefile, 'wb') as f:\n",
    "        pickle.dump(recs, f)\n",
    "\n",
    "\n",
    "def generate_dataset(basepath: str, config: str, history_size: int) -> None:\n",
    "    recs = load_symbolic(basepath)\n",
    "    cachepath = os.path.join(basepath, 'dataset_caches')\n",
    "\n",
    "    def write_frame(s, ts, tk, fr, graphs_to_write):\n",
    "        # Write cache file\n",
    "        os.makedirs(os.path.join(cachepath, config, s, ts, tk), exist_ok=True)\n",
    "        for side in ['left', 'right']:\n",
    "            cachefile = os.path.join(cachepath, config, s, ts, tk, 'frame_{}_{}.cache'.format(fr, side))\n",
    "            written = False\n",
    "            while not written:\n",
    "                try:\n",
    "                    with open(cachefile, 'wb') as f:\n",
    "                        pickle.dump(graphs_to_write[side], f)\n",
    "                except IOError:\n",
    "                    time.sleep(0.25)\n",
    "                else:\n",
    "                    written = True\n",
    "\n",
    "    for subject, task, take in crawl_dataset():\n",
    "        recording: Recording = recs[subject][task][take]\n",
    "        recording.check_integrity()\n",
    "        for i in range(0, recording.frame_count, 1):\n",
    "            sgl = recording.to_scene_graphs(i, history_size=history_size)\n",
    "            scene_graph = flatten_scene_graphs(sgl)\n",
    "            scene_graph.check_integrity()\n",
    "            graphs = {\n",
    "                'right': scene_graph.to_data_dict(mirrored=False),\n",
    "                'left': scene_graph.to_data_dict(mirrored=True)\n",
    "            }\n",
    "            write_frame(subject, task, take, i, graphs)\n",
    "\n",
    "def generate_dict():\n",
    "    dict = {}\n",
    "    dict['left_action'] = []\n",
    "    dict['right_action'] = []\n",
    "\n",
    "    dict['object'] = []\n",
    "    dict['bonuding_box']\n",
    "\n",
    "    dict['relationship']\n",
    "\n",
    "    dict['left_hand']\n",
    "    dict['right_hand']\n",
    "\n",
    "    return dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "index = 1\n",
    "type = \"3d_objects\"\n",
    "derived_data_path = '/home/yuxuan/project/Bimanual_Action_Recognition/KIT_BIMACS_DATASET/bimacs_derived_data/subject_2/task_2_k_cooking_with_bowls/take_1'\n",
    "\n",
    "with open(os.path.join(derived_data_path, type, 'frame_{}.json'.format(index))) as of:\n",
    "    # path example for _load_objects(): derived_data_path + \"3d_objects\" + frame_10\n",
    "    x = json.load(of)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'bounding_box': {'x0': 168.26443481445312,\n   'x1': 239.86705017089844,\n   'y0': -799.8482666015625,\n   'y1': -747.72216796875,\n   'z0': -1073.76806640625,\n   'z1': -956.6685180664062},\n  'certainty': 0.9999489784240723,\n  'class_index': 9,\n  'class_name': 'cup',\n  'colour': [47, 255, 0],\n  'instance_name': 'cup_1',\n  'past_bounding_box': {'x0': 168.42893981933594,\n   'x1': 239.86705017089844,\n   'y0': -799.900146484375,\n   'y1': -783.9974365234375,\n   'z0': -1073.76806640625,\n   'z1': -954.970703125}},\n {'bounding_box': {'x0': 57.41846466064453,\n   'x1': 114.52205657958984,\n   'y0': -792.9141235351562,\n   'y1': -721.594970703125,\n   'z0': -1091.5546875,\n   'z1': -1021.260009765625},\n  'certainty': 0.9998663663864136,\n  'class_index': 9,\n  'class_name': 'cup',\n  'colour': [47, 255, 0],\n  'instance_name': 'cup_2',\n  'past_bounding_box': {'x0': 61.894004821777344,\n   'x1': 112.54142761230469,\n   'y0': -785.53125,\n   'y1': -724.8842163085938,\n   'z0': -1089.0076904296875,\n   'z1': -1045.90966796875}},\n {'bounding_box': {'x0': 107.10331726074219,\n   'x1': 316.31976318359375,\n   'y0': -801.1397705078125,\n   'y1': -653.3648681640625,\n   'z0': -1596.264892578125,\n   'z1': -1298.6112060546875},\n  'certainty': 0.9997420907020569,\n  'class_index': 1,\n  'class_name': 'bowl',\n  'colour': [255, 0, 175],\n  'instance_name': 'bowl_3',\n  'past_bounding_box': {'x0': 108.38401794433594,\n   'x1': 316.31976318359375,\n   'y0': -802.6801147460938,\n   'y1': -653.692138671875,\n   'z0': -1645.2132568359375,\n   'z1': -1298.4617919921875}},\n {'bounding_box': {'x0': -157.61090087890625,\n   'x1': 62.066184997558594,\n   'y0': -796.907470703125,\n   'y1': -662.678955078125,\n   'z0': -1557.2073974609375,\n   'z1': -1417.404541015625},\n  'certainty': 0.9940598607063293,\n  'class_index': 1,\n  'class_name': 'bowl',\n  'colour': [255, 0, 175],\n  'instance_name': 'bowl_4',\n  'past_bounding_box': {'x0': -157.3478240966797,\n   'x1': 61.9073371887207,\n   'y0': -795.4816284179688,\n   'y1': -664.4202880859375,\n   'z0': -1557.6090087890625,\n   'z1': -1418.366455078125}},\n {'bounding_box': {'x0': -98.41077423095703,\n   'x1': -15.795063972473145,\n   'y0': -753.189453125,\n   'y1': -681.294677734375,\n   'z0': -2349.364501953125,\n   'z1': -2237.958984375},\n  'certainty': 1.0,\n  'class_index': 0,\n  'class_name': 'RightHand',\n  'colour': [199, 7, 247],\n  'instance_name': 'RightHand_5',\n  'past_bounding_box': {'x0': -118.70349884033203,\n   'x1': 3.7927799224853516,\n   'y0': -791.1263427734375,\n   'y1': -690.2666625976562,\n   'z0': -2380.123046875,\n   'z1': -2232.55322265625}},\n {'bounding_box': {'x0': 3.1139285564422607,\n   'x1': 94.18759155273438,\n   'y0': -758.1732177734375,\n   'y1': -633.3220825195312,\n   'z0': -2377.92041015625,\n   'z1': -2255.83251953125},\n  'certainty': 1.0,\n  'class_index': 0,\n  'class_name': 'LeftHand',\n  'colour': [199, 7, 247],\n  'instance_name': 'LeftHand_6',\n  'past_bounding_box': {'x0': 18.963899612426758,\n   'x1': 114.74671936035156,\n   'y0': -733.0446166992188,\n   'y1': -621.534912109375,\n   'z0': -2390.877197265625,\n   'z1': -2263.81494140625}},\n {'bounding_box': {'x0': -365.4077453613281,\n   'x1': -248.00193786621094,\n   'y0': -811.5205688476562,\n   'y1': -769.4779663085938,\n   'z0': -1638.7779541015625,\n   'z1': -1309.4849853515625},\n  'certainty': 0.9568049907684326,\n  'class_index': 5,\n  'class_name': 'whisk',\n  'colour': [255, 143, 0],\n  'instance_name': 'whisk_7',\n  'past_bounding_box': {'x0': -365.4077453613281,\n   'x1': -248.00193786621094,\n   'y0': -811.5205688476562,\n   'y1': -769.4779663085938,\n   'z0': -1638.7779541015625,\n   'z1': -1309.4849853515625}}]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "derived_data = 'bimacs_derived_data'\n",
    "rgbd_data_ground_truth = 'bimacs_rgbd_data_ground_truth'\n",
    "basepath = '/home/yuxuan/project/Bimanual_Action_Recognition/Processed'\n",
    "dataset_path = '/home/yuxuan/project/Bimanual_Action_Recognition/KIT_BIMACS_DATASET'\n",
    "\n",
    "\n",
    "# os.makedirs(os.path.join(basepath, 'Information'), exist_ok=True)\n",
    "# cachefile = os.path.join(basepath, 'dataset_caches', 'symbolic_dataset.cache')\n",
    "recs = {}\n",
    "\n",
    "derived_data_paths, ground_truth_paths = get_raw_dataset_paths(os.path.join(dataset_path, derived_data),\n",
    "                                         os.path.join(dataset_path, rgbd_data_ground_truth))\n",
    "\n",
    "# load derived data into a object\n",
    "for derived_data_path, ground_truth_path in zip(derived_data_paths, ground_truth_paths):\n",
    "    *_, subject, task, take = derived_data_path.split('/')\n",
    "    if subject not in recs: # create new subject\n",
    "        recs[subject] = {}\n",
    "    if task not in recs[subject]: # create new task\n",
    "        recs[subject][task] = {}\n",
    "\n",
    "    rec = Recording(derived_data_path, ground_truth_path)\n",
    "    recs[subject][task][take] = rec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "'saw_1'"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec.frame_count # how many frames does the take have\n",
    "rec.ground_truth_left.gtlist # left hand groundtruth action of the take\n",
    "rec.ground_truth_right.gtlist # right hand groundtruth action of the take\n",
    "\n",
    "rec.relations[0][0].relation_name # first index means frame, second index means number of relation in this frame\n",
    "rec.objects[0][0].instance_name # first index means frame, second index means number of object in the frame"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}